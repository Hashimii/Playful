{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yield prediction exploration\n",
    "\n",
    "Goal of this exploration is to find a model that can predict the yield of a PV-system based on:\n",
    "\n",
    "- installation data\n",
    "- weather data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required libraries\n",
    "\n",
    "- [pandas](https://pypi.org/project/pandas/)\n",
    "- [pgeocode](https://pypi.org/project/pgeocode/)\n",
    "- [geopandas](https://pypi.org/project/geopandas/)\n",
    "- [matplotlib](https://pypi.org/project/matplotlib/)\n",
    "- [scikit-learn](https://pypi.org/project/scikit-learn/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install pgeocode\n",
    "%pip install geopandas\n",
    "%pip install matplotlib\n",
    "%pip install scikit-learn\n",
    "%pip install openpyxl\n",
    "%pip install geopy\n",
    "%pip install suncalc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pgeocode\n",
    "from geopy import distance\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic setup\n",
    "\n",
    "To make things easier, we will first define some constants and functions that we will use throughout this notebook.\n",
    "\n",
    "### Constants\n",
    "\n",
    "- `DATA_DIR`: directory where the data is stored\n",
    "- `GENERAL_INSTALLATION_DATA`: file path of the general installation data\n",
    "- `GENERAL_WEATHER_STATION_DATA`: file path of the general weather station data\n",
    "- `INSTALLATION_YIELD_DATA`: file path of the installation yield data\n",
    "- `WEATHER_STATION_DATA`: file path of the weather station data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Constants\n",
    "\n",
    "DATA_DIR = os.path.join(\"..\", \"data\")\n",
    "GENERAL_INSTALLATION_DATA = os.path.join(DATA_DIR, \"general_installation_data.xlsx\")\n",
    "GENERAL_WEATHER_STATION_DATA = os.path.join(DATA_DIR, \"general_weather_station_data.xlsx\")\n",
    "INSTALLATION_YIELD_DATA = os.path.join(DATA_DIR, \"installation_yield_data.xlsx\")\n",
    "WEATHER_STATION_DATA = os.path.join(DATA_DIR, \"weather_station_data.xlsx\")\n",
    "\n",
    "NETHERLANDS_MAP_DATA = os.path.join(DATA_DIR, \"map\", \"gadm41_NLD_2.shp\")\n",
    "\n",
    "DATA_SET = os.path.join(DATA_DIR, \"dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "### General installation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_installation_data = pd.read_excel(GENERAL_INSTALLATION_DATA)\n",
    "\n",
    "general_installation_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the first 5 rows of the general installation data already see that we have quite installation data available. The most interesting columns right now are:\n",
    "\n",
    "- `id`: unique identifier of the installation\n",
    "- `zipcode`: zipcode of the installation\n",
    "- `weather_station_id`: id of the weather station that is closest to the installation\n",
    "- `geometric_yield`: UNKNOWN\n",
    "- `orientation_deg_north`: orientation of the installation in degrees\n",
    "- `roof_inclination`: inclination of the installation in degrees\n",
    "\n",
    "We also see a few columns that look like they are either a constant (e.g. `efficiency`), or can be calculated based on other columns (e.g. `total_output` as `number_of_panels` * `output`). Lets validate if this assumptions are correct and remove the columns that are not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_installation_data[\"efficiency\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the efficiency column, we see that it is indeed a constant. We can remove this column since its not relevant for our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_installation_data.drop(columns=[\"efficiency\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have a column that is not named (`Unnamed: 19`), lets see, if this column contains any useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_installation_data[\"Unnamed: 19\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is also a constant, we can just drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_installation_data.drop(columns=[\"Unnamed: 19\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the columns that looked like its calculated was the `total_output` column. Lets see if this is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_total_output = general_installation_data[\"number_of_panels\"] * general_installation_data[\"output\"]\n",
    "\n",
    "calculated_total_output.equals(general_installation_data[\"total_output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the total output can be calculated based on the number of panels and the output per panel. We can remove this column since its not relevant for our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_installation_data.drop(columns=[\"total_output\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_total_inverter_output = general_installation_data[\"amount\"] * general_installation_data[\"power\"]\n",
    "\n",
    "calculated_total_inverter_output.equals(general_installation_data[\"total_inverter_output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also the `total_inverter_output` is just a calcualted column. We can remove this column since its not relevant for our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_installation_data.drop(columns=[\"total_inverter_output\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have only the interesting columns left, lets take a look at the data to see, if we need to do some more cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_installation_data.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have no missing values in the columns that we are interested in.\n",
    "\n",
    "Lets take a look at the columns and see, if we can derive some more information from them (do some feature engineering).\n",
    "\n",
    "The `zipcode` column looks like it contains the zipcode of the installation. Since it is a categorial value, we would have to create dummies for each zipcode. This would result in a lot of columns. We can use the `pgeocode` library to get the latitude and longitude of the zipcode. This would result in 2 new columns, which is much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nomi = pgeocode.Nominatim('nl')\n",
    "\n",
    "# Get the location from a postcode\n",
    "def get_location_from_postcode(postcode: str):\n",
    "\n",
    "    code = postcode[:4]\n",
    "\n",
    "    result =  nomi.query_postal_code(code)\n",
    "\n",
    "    return pd.Series({\n",
    "        \"latitude\": result.latitude,\n",
    "        \"longitude\": result.longitude,\n",
    "    })\n",
    "    \n",
    "installation_locations = general_installation_data.apply(lambda x: get_location_from_postcode(x['zipcode']), axis=1)\n",
    "\n",
    "general_installation_data = pd.concat([general_installation_data, installation_locations], axis=1)\n",
    "\n",
    "general_installation_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the location is not exactly the location of the installation, it should be close enough for our prediction.\n",
    "\n",
    "Since we have the latitude and longitude of the installation, we can now remove the `zipcode` column and take a look at where the installations are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_installation_data.drop(columns=[\"zipcode\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netherlands_map = gpd.read_file(NETHERLANDS_MAP_DATA)\n",
    "\n",
    "general_installation_data_locations = gpd.GeoDataFrame(\n",
    "    general_installation_data, \n",
    "    geometry=gpd.points_from_xy(\n",
    "        general_installation_data[\"longitude\"], \n",
    "        general_installation_data[\"latitude\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "netherlands_map.plot(ax=ax, color='lightgrey')\n",
    "general_installation_data_locations.plot(ax=ax, color='red', marker='o', markersize=10)\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.title(\"Installation locations in the Netherlands\")\n",
    "plt.savefig(\"installation_locations.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General weather station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_weather_station_data = pd.read_excel(GENERAL_WEATHER_STATION_DATA)\n",
    "\n",
    "general_weather_station_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the first 5 rows of the general weather station data we see, that we have not a lot of data here. But since we have the latitude and longitude of the weather stations, we can plot them on a map together with the installations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_weather_station_data_locations = gpd.GeoDataFrame(\n",
    "    general_weather_station_data, \n",
    "    geometry=gpd.points_from_xy(\n",
    "        general_weather_station_data[\"lon\"], \n",
    "        general_weather_station_data[\"lat\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "netherlands_map.plot(ax=ax, color='lightgrey')\n",
    "general_installation_data_locations.plot(ax=ax, color='gray', marker='o', markersize=10)\n",
    "general_weather_station_data_locations.plot(ax=ax, color='blue', marker='x', markersize=10)\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.title(\"Installation & weather station locations in the Netherlands\")\n",
    "plt.savefig(\"installation_weather_station_locations.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the weather station data and the installation data both have latitude and longitude, we can calculate the distance between them to get a new feature that might be useful for our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_to_weather_station(weather_station_id: str, latitude: float, longitude: float):\n",
    "    weather_station = general_weather_station_data[general_weather_station_data[\"id\"] == weather_station_id].iloc[0]\n",
    "\n",
    "    return distance.distance([latitude, longitude], [weather_station[\"lat\"], weather_station[\"lon\"]]).m\n",
    "\n",
    "\n",
    "general_installation_data[\"weather_station_distance_m\"] = general_installation_data_locations.apply(\n",
    "    lambda x: calculate_distance_to_weather_station(x[\"weather_station_id\"], x[\"latitude\"], x[\"longitude\"]), axis=1\n",
    ")\n",
    "\n",
    "general_installation_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station_data = pd.read_excel(WEATHER_STATION_DATA)\n",
    "\n",
    "weather_station_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data, we have quite some cleaning to do. Lets first transform the `time_start` into a datetime object and create a new column `date` that contains only the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station_data[\"date\"] = pd.to_datetime(weather_station_data[\"time_start\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "weather_station_data.drop(columns=[\"time_start\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at the temperature, we see that this can not be °C. It seems like the temperature has been multiplied by 10. Lets fix this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station_data[\"min_temperature\"] = weather_station_data[\"min_temperature\"] / 10\n",
    "weather_station_data[\"max_temperature\"] = weather_station_data[\"max_temperature\"] / 10\n",
    "weather_station_data[\"average_temperature\"] = weather_station_data[\"average_temperature\"] / 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see if we have any missing values in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station_data.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the entries do not have any values for the weather. We have two options now:\n",
    "\n",
    "1. Remove the entries without weather data and don't train the model on these entries\n",
    "2. Try to come up with a way to fill the missing values\n",
    "\n",
    "We will for now just remove the entries without weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station_data.dropna(inplace=True)\n",
    "\n",
    "weather_station_data.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have only entries with weather data left, lets take a look at the installation yield data.\n",
    "\n",
    "## Installation yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "installation_yield_data = pd.read_excel(INSTALLATION_YIELD_DATA)\n",
    "\n",
    "installation_yield_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The installation yield data requires quite some cleaning, lets go for it. Lets start by renaming the columns to something more meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "installation_yield_data.rename(\n",
    "    columns={\n",
    "        \"Day of Periode 2\": \"date\", \n",
    "        \"Unnamed: 3\": \"measured_yield\"\n",
    "    }, \n",
    "    inplace=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets transform the `date` column into a datetime object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "installation_yield_data[\"date\"] = pd.to_datetime(installation_yield_data[\"date\"], format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, lets check for missing values. In this case, we can directly drop the rows with missing values since it does not make sense to somehow try to fill the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "installation_yield_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data now is ready and we can start creating the dataset that we will use for training the model.\n",
    "\n",
    "## Create dataset\n",
    "\n",
    "In the steps above, we have cleaned the data and created some new features. Now we can create the dataset that we will use for training the model.\n",
    "\n",
    "To do so, we will merge all cleaned dataframes together. This can be done using the `id` column and the `weather_station_id` + `date` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.merge(\n",
    "    installation_yield_data, \n",
    "    general_installation_data, \n",
    "    how=\"left\", \n",
    "    left_on=[\"id\"], \n",
    "    right_on=[\"id\"]\n",
    "    ).merge(\n",
    "        weather_station_data,\n",
    "        how=\"left\",\n",
    "        left_on=[\"weather_station_id\", \"date\"],\n",
    "        right_on=[\"weather_station_id\", \"date\"],\n",
    "        suffixes=(\"\", \"_weather\")\n",
    "    )\n",
    "\n",
    "dataset_index = pd.MultiIndex.from_frame(\n",
    "    dataset[[\"id\", \"date\"]],\n",
    "    names=[\"id\", \"date\"]\n",
    ")\n",
    "\n",
    "dataset.set_index(dataset_index, inplace=True)\n",
    "\n",
    "dataset.dropna(inplace=True)\n",
    "\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our model is not able to handle categorial values, we will create dummies for the columns containing categorial values we want to use and remove all categorial columns from the dataset afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.get_dummies(dataset, columns=[\"panel_brand\", \"panel_model\", \"inverter_brand\", \"inverter_model\"])\n",
    "\n",
    "dataset.drop(columns=[\"id\", \"pchn_x\", \"pchn_y\", \"weather_station_id\", \"name\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there is only one column left that can not be used for training our model: the `date` column. But this column holds some very valuable data. For this reason, we need to do some form of feature engineering to make this data usable for our model.\n",
    "\n",
    "__Why the data is interesting?__\n",
    "\n",
    "A key factor that determines the yield of the PV-system is the angle of the sun rays hitting the panels. The angle of the sun rays depends on the location, the time of the day and the time of the year. Since we have the date of the measurement, we can calculate the angle of the sun rays hitting the panels. After calculating the angle, we can remove the `date` column as well as the `latitude` and `longitude` columns and the `geometric_yield`, `orientation_deg_north` and `roof_inclination` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from suncalc import get_position, get_times\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def calculate_naive_average_sun_position(year: int, month: int, day: int, latitude: float, longitude: float):\n",
    "    times = get_times(date=datetime(year, month, day, 0, 0, 0), lng=longitude, lat=latitude)\n",
    "\n",
    "    positions = []\n",
    "\n",
    "    time = times['sunrise']\n",
    "    end = times['sunset']\n",
    "\n",
    "    azimuth_sum = 0\n",
    "    altitude_sum = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    while time <= end:\n",
    "        position = get_position(date=time, lng=longitude, lat=latitude)\n",
    "\n",
    "        azimuth = position['azimuth']\n",
    "        altitude = position['altitude']\n",
    "\n",
    "        positions.append({\"datetime\": time, \"azimuth\": azimuth, \"altitude\": altitude})\n",
    "\n",
    "        azimuth_sum += azimuth\n",
    "        altitude_sum += altitude\n",
    "        num_samples += 1\n",
    "\n",
    "        time += timedelta(minutes=10)\n",
    "\n",
    "    average_azimuth = azimuth_sum / num_samples\n",
    "    average_altitude = altitude_sum / num_samples\n",
    "\n",
    "    return pd.Series({\n",
    "        \"average_azimuth\": average_azimuth,\n",
    "        \"average_altitude\": average_altitude,\n",
    "    })\n",
    "\n",
    "naive_sun_position = dataset.apply(\n",
    "    lambda x: calculate_naive_average_sun_position(\n",
    "        year=x[\"date\"].year, \n",
    "        month=x[\"date\"].month,\n",
    "        day=x[\"date\"].day,\n",
    "        latitude=x[\"latitude\"], \n",
    "        longitude=x[\"longitude\"]\n",
    "    ), axis=1)\n",
    "\n",
    "dataset = pd.concat([dataset, naive_sun_position], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not that we have translated the date into the average sun position, we can remove the `date` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(columns=[\"date\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have dropped the `date` column, we should have only numerical columns left. Lets check if this is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! We only have bools, floats and integers left. The dataset is now ready to be used for training the model.\n",
    "\n",
    "Since the preprocessing process takes quite some time, we want to save the cleaned dataset to a file. This way, we can just load the cleaned dataset from the file instead of having to run the preprocessing process every time we want to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(DATA_SET, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "With the now cleaned data, we can start training a model. We will use the [MLPRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html) from [scikit-learn](https://scikit-learn.org/stable/index.html). To identify the best parameters for the model, we will use [GridSearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to try out different combinations of parameters.\n",
    "\n",
    "## Create training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "dataset = pd.read_csv(DATA_SET, parse_dates=[\"date\"])\n",
    "\n",
    "dataset_index = pd.MultiIndex.from_frame(\n",
    "    dataset[[\"id\", \"date\"]],\n",
    "    names=[\"id\", \"date\"]\n",
    ")\n",
    "\n",
    "dataset.set_index(dataset_index, inplace=True)\n",
    "dataset.drop(columns=[\"id\", \"date\"], inplace=True)\n",
    "\n",
    "dataset.dropna(inplace=True)\n",
    "\n",
    "X_scaler = StandardScaler()\n",
    "X = dataset.drop(columns=[\"measured_yield\"])\n",
    "X = X_scaler.fit_transform(X)\n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "y = dataset[\"measured_yield\"]\n",
    "y = y_scaler.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have out train and test data split, we have to define the possible parameters for the model. GridSearch will then try out all possible combinations of the parameters and return the best combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "        \"activation\": [\"logistic\", \"tanh\", \"relu\"],\n",
    "        \"solver\": [\"lbfgs\", \"sgd\", \"adam\"],\n",
    "        \"max_iter\": [1_000, 2_000, 3_000, 5_000, 10_000],\n",
    "        \"hidden_layer_sizes\": [\n",
    "            (1,),\n",
    "            (2,),\n",
    "            (4,),\n",
    "            (8,),\n",
    "            (16,),\n",
    "            (32,),\n",
    "            (64,),\n",
    "        ],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the parameters defined, we can now run the GridSearch and see what the best parameters are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=MLPRegressor(),\n",
    "    param_grid=param_grid,\n",
    "    verbose=2,\n",
    "    n_jobs=5\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the GridSearch, we now want to save the result to a file so that we can load them later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"best_params.txt\", \"w\") as f:\n",
    "    f.write(str(grid_search.best_params_))\n",
    "\n",
    "with open(\"best_estimator.txt\", \"w\") as f:\n",
    "    f.write(str(grid_search.best_estimator_))\n",
    "\n",
    "with open(\"best_score.txt\", \"w\") as f:\n",
    "    f.write(str(grid_search.best_score_))\n",
    "\n",
    "with open(\"cv_results.txt\", \"w\") as f:\n",
    "    f.write(str(grid_search.cv_results_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the best parameters identified, we can now train the model with the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "with open(\"best_params.txt\", mode=\"r\") as f:\n",
    "        best_params = eval(f.read())\n",
    "\n",
    "\n",
    "model = MLPRegressor(\n",
    "    activation=best_params[\"activation\"],\n",
    "    hidden_layer_sizes=best_params[\"hidden_layer_sizes\"],\n",
    "    max_iter=best_params[\"max_iter\"],\n",
    "    solver=best_params[\"solver\"],\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because training takes quite a while, we want to save the trained model to a file so that we can load it later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(model, \"model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how the model performs, we will use the test data to predict the yield and compare it to the actual yield."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "\n",
    "model = joblib.load(\"model.joblib\")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "f\"Root mean squared error: {rmse}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better understanding of how the model performs, we will plot the predicted yield vs the actual yield vs the calculated yield (using the formula that Wocozon is using right now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)\n",
    "\n",
    "rmse = mean_squared_error(y, y_pred, squared=False)\n",
    "\n",
    "f\"Root mean squared error: {rmse}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_yield(\n",
    "    global_solar_radiation: float,\n",
    "    number_of_panels: int,\n",
    "    output: float,\n",
    "    efficiency: float,\n",
    "    geometric_yield: float,\n",
    "    radiation_freedom: float,\n",
    "):\n",
    "    return round((global_solar_radiation/360 * number_of_panels * output * efficiency * geometric_yield * radiation_freedom)/1000, 2)\n",
    "\n",
    "y_calculated = dataset.apply(\n",
    "    lambda x: calculate_yield(\n",
    "        global_solar_radiation=x[\"global_solar_radiation\"],\n",
    "        number_of_panels=x[\"number_of_panels\"],\n",
    "        output=x[\"output\"],\n",
    "        efficiency=0.86, # `efficiency` was one of the columns in the original dataset, but it was always 0.86\n",
    "        geometric_yield=x[\"geometric_yield\"],\n",
    "        radiation_freedom=x[\"radiation_freedom\"],\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "rmse = mean_squared_error(dataset[\"measured_yield\"], y_calculated, squared=False)\n",
    "\n",
    "f\"Root mean squared error: {rmse}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the measured yield, the predicted yield and the calculated yield, we can now visualize the difference between them. This requires some work, because we have scaled the data before training the model. We need to reverse the scaling to get the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_scaler.inverse_transform(y_pred.reshape(-1, 1))\n",
    "\n",
    "dataset[\"predicted_yield\"] = y_pred\n",
    "dataset[\"calculated_yield\"] = y_calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids = dataset.index.get_level_values(\"id\").unique()\n",
    "\n",
    "unique_ids = unique_ids[:10]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(unique_ids), figsize=(40, 20), sharex=True)\n",
    "\n",
    "for index, id in enumerate(unique_ids):\n",
    "    data = dataset.loc[id]\n",
    "\n",
    "    axes[index].plot(data.index.get_level_values(\"date\"), data[\"measured_yield\"], label=\"Measured yield\")\n",
    "    axes[index].plot(data.index.get_level_values(\"date\"), data[\"calculated_yield\"], label=\"Calculated yield\")\n",
    "    axes[index].plot(data.index.get_level_values(\"date\"), data[\"predicted_yield\"], label=\"Predicted yield\")\n",
    "    axes[index].set_title(f\"ID: {id}\")\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(\"plots.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the plot, the resuts look very promising. The predicted yield is very close to the measured yield and far more precise than the calculated yield."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(filename='./plots.png', embed=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably the predictions can be improved even more my tweaking the parameters or by using a different model. But for now, this is good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=len(unique_ids), figsize=(40, 20), sharex=True)\n",
    "\n",
    "for index, id in enumerate(unique_ids):\n",
    "    data = dataset.loc[id]\n",
    "\n",
    "    measured_yield_error = data[\"measured_yield\"] - data[\"measured_yield\"]\n",
    "    calculated_yield_error = data[\"calculated_yield\"] - data[\"measured_yield\"]\n",
    "    predicted_yield_error = data[\"predicted_yield\"] - data[\"measured_yield\"]\n",
    "\n",
    "    axes[index].plot(data.index.get_level_values(\"date\"), measured_yield_error, label=\"Measured yield error\")\n",
    "    axes[index].plot(data.index.get_level_values(\"date\"), calculated_yield_error, label=\"Calculated yield error\")\n",
    "    axes[index].plot(data.index.get_level_values(\"date\"), predicted_yield_error, label=\"Predicted yield error\")\n",
    "    axes[index].set_title(f\"ID: {id}\")\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(\"error-plots.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(filename='./error-plots.png', embed=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "\n",
    "\n",
    "def calculate_monthly_average(x: pd.Series):\n",
    "    date = x[\"date\"]\n",
    "\n",
    "    month = date.month\n",
    "    year = date.year\n",
    "\n",
    "    days_in_month = calendar.monthrange(year, month)[1]\n",
    "\n",
    "    return pd.Series({\n",
    "        \"average_measured_yield\": x[\"measured_yield\"] / days_in_month,\n",
    "        \"average_calculated_yield\": x[\"calculated_yield\"] / days_in_month,\n",
    "        \"average_predicted_yield\": x[\"predicted_yield\"] / days_in_month,\n",
    "    })\n",
    "\n",
    "\n",
    "grouped_df = dataset.groupby([pd.Grouper(level='id'), pd.Grouper(level='date', freq='M')]).sum().reset_index()\n",
    "\n",
    "selected_id = 70744\n",
    "\n",
    "\n",
    "selected_dataset = grouped_df[grouped_df[\"id\"] == selected_id]\n",
    "\n",
    "average_yields = selected_dataset.apply(\n",
    "    lambda x: calculate_monthly_average(\n",
    "       x\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "selected_dataset = pd.concat([selected_dataset, average_yields], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"default\")\n",
    "\n",
    "font = {\n",
    "    'weight': 'bold',\n",
    "    'size' : 22\n",
    "}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(40, 15))\n",
    "\n",
    "ax.fill_between(selected_dataset[\"date\"], selected_dataset[\"average_measured_yield\"], alpha=0.3)\n",
    "\n",
    "ax.plot(selected_dataset[\"date\"], selected_dataset[\"average_calculated_yield\"], label=\"Calculated yield\", color=\"red\", linewidth=4)\n",
    "\n",
    "ax.plot(selected_dataset[\"date\"], selected_dataset[\"average_predicted_yield\"], label=\"Predicted yield\", color=\"green\", linewidth=4)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Average monthly yield (kWh)\")\n",
    "plt.title(f\"Monthly average yield for installation ID: {selected_id}\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "fig.savefig(\"grouped.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(filename='./grouped.png', embed=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
